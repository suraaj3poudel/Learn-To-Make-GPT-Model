#!/usr/bin/env python3
"""
Generate Phase 3 and Phase 4 notebooks
Complete Transformer and GPT implementation notebooks
"""
import json
import os

def save_notebook(filepath, cells):
    """Save properly formatted Jupyter notebook"""
    notebook = {
        "cells": cells,
        "metadata": {
            "kernelspec": {
                "display_name": "Python 3",
                "language": "python",
                "name": "python3"
            },
            "language_info": {
                "codemirror_mode": {"name": "ipython", "version": 3},
                "file_extension": ".py",
                "mimetype": "text/x-python",
                "name": "python",
                "nbconvert_exporter": "python",
                "pygments_lexer": "ipython3",
                "version": "3.9.6"
            }
        },
        "nbformat": 4,
        "nbformat_minor": 5
    }
    
    os.makedirs(os.path.dirname(filepath), exist_ok=True)
    with open(filepath, 'w') as f:
        json.dump(notebook, f, indent=2)
    print(f"‚úÖ Created: {filepath}")

def md(text): return {"cell_type": "markdown", "metadata": {}, "source": text.split('\n')}
def code(text): return {"cell_type": "code", "metadata": {}, "execution_count": None, "outputs": [], "source": text.split('\n')}

print("="*60)
print("Creating Phase 3 & 4 Notebooks")
print("="*60)

# ============================================================================
# PHASE 3, LESSON 1: TRANSFORMER ARCHITECTURE
# ============================================================================
print("\nüìù Creating Phase 3, Lesson 1: Transformer Architecture...")

phase3_lesson1 = [
    md("""# Phase 3, Lesson 1: Transformer Architecture

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/suraaj3poudel/Learn-To-Make-GPT-Model/blob/main/phase3_mini_transformer/01_transformer_architecture.ipynb)

Understanding "Attention is All You Need" üìÑ

## What You'll Learn
1. Complete Transformer architecture
2. Multi-head attention in detail
3. Position encodings  
4. Feed-forward networks
5. Layer normalization

The architecture that changed everything!"""),

    code("""# Setup
import numpy as np
import matplotlib.pyplot as plt
import math

print('‚úÖ Ready to build Transformers!')"""),

    md("""## 1. Transformer Overview

The Transformer architecture has two main parts:

1. **Encoder**: Processes input sequence
2. **Decoder**: Generates output sequence

For GPT, we only use the **Decoder** (autoregressive generation).

**Key Components**:
- Multi-head self-attention
- Position encodings
- Feed-forward networks
- Layer normalization
- Residual connections

Let's build each part!"""),

    md("""## 2. Positional Encoding

**Problem**: Attention has no sense of word order!
- "I love you" vs "You love I" ‚Üí Same attention patterns!

**Solution**: Add positional information to embeddings

**Formula** (sinusoidal):
```
PE(pos, 2i) = sin(pos / 10000^(2i/d))
PE(pos, 2i+1) = cos(pos / 10000^(2i/d))
```

Where `pos` = position, `i` = dimension index"""),

    code("""def positional_encoding(max_len, d_model):
    \"\"\"
    Create positional encoding matrix
    
    Args:
        max_len: Maximum sequence length
        d_model: Model dimension
    
    Returns:
        pe: (max_len, d_model) positional encodings
    \"\"\"
    pe = np.zeros((max_len, d_model))
    
    for pos in range(max_len):
        for i in range(0, d_model, 2):
            # Sin for even indices
            pe[pos, i] = math.sin(pos / (10000 ** (2 * i / d_model)))
            
            # Cos for odd indices
            if i + 1 < d_model:
                pe[pos, i + 1] = math.cos(pos / (10000 ** (2 * i / d_model)))
    
    return pe

# Create positional encodings
max_len = 50
d_model = 64

pe = positional_encoding(max_len, d_model)

print(f"Positional encoding shape: {pe.shape}")

# Visualize
plt.figure(figsize=(12, 6))
plt.imshow(pe, cmap='RdBu', aspect='auto')
plt.colorbar(label='Encoding value')
plt.xlabel('Dimension')
plt.ylabel('Position')
plt.title('Positional Encoding Visualization')
plt.tight_layout()
plt.show()

print("Each position gets a unique pattern!")
print("Similar positions have similar encodings")"""),

    md("""## 3. Multi-Head Attention

Instead of one attention mechanism, use multiple "heads"!

Each head:
- Has its own Q, K, V projections
- Attends to different aspects of the input
- Produces partial output

Then concatenate all heads and project."""),

    code("""class MultiHeadAttention:
    def __init__(self, d_model, num_heads):
        assert d_model % num_heads == 0, "d_model must be divisible by num_heads"
        
        self.d_model = d_model
        self.num_heads = num_heads
        self.d_k = d_model // num_heads  # Dimension per head
        
        # Weight matrices for Q, K, V projections
        self.W_q = np.random.randn(d_model, d_model) * 0.01
        self.W_k = np.random.randn(d_model, d_model) * 0.01
        self.W_v = np.random.randn(d_model, d_model) * 0.01
        self.W_o = np.random.randn(d_model, d_model) * 0.01  # Output projection
    
    def split_heads(self, x):
        \"\"\"Split into multiple heads\"\"\"
        batch_size, seq_len, d_model = x.shape[0], x.shape[0], x.shape[1]
        # Reshape: (seq_len, d_model) -> (seq_len, num_heads, d_k)
        x = x.reshape(seq_len, self.num_heads, self.d_k)
        # Transpose: (num_heads, seq_len, d_k)
        return x.transpose(1, 0, 2)
    
    def scaled_dot_product_attention(self, Q, K, V, mask=None):
        \"\"\"Compute attention for one head\"\"\"
        d_k = Q.shape[-1]
        
        # Attention scores
        scores = np.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
        
        # Apply mask if provided (for decoder)
        if mask is not None:
            scores = scores + (mask * -1e9)
        
        # Softmax
        exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
        attention_weights = exp_scores / exp_scores.sum(axis=-1, keepdims=True)
        
        # Apply to values
        output = np.matmul(attention_weights, V)
        
        return output, attention_weights
    
    def forward(self, x, mask=None):
        \"\"\"
        Multi-head attention forward pass
        
        Args:
            x: (seq_len, d_model) input
            mask: Optional mask for decoder
        
        Returns:
            output: (seq_len, d_model)
        \"\"\"
        seq_len = x.shape[0]
        
        # Linear projections
        Q = np.dot(x, self.W_q)
        K = np.dot(x, self.W_k)
        V = np.dot(x, self.W_v)
        
        # Split into heads
        Q = self.split_heads(Q[None, :, :])[0]  # (num_heads, seq_len, d_k)
        K = self.split_heads(K[None, :, :])[0]
        V = self.split_heads(V[None, :, :])[0]
        
        # Apply attention for each head
        head_outputs = []
        for i in range(self.num_heads):
            head_out, _ = self.scaled_dot_product_attention(
                Q[i:i+1], K[i:i+1], V[i:i+1], mask
            )
            head_outputs.append(head_out[0])
        
        # Concatenate heads
        concat = np.concatenate(head_outputs, axis=-1)
        
        # Final linear projection
        output = np.dot(concat, self.W_o)
        
        return output

# Test multi-head attention
d_model = 64
num_heads = 8
seq_len = 10

mha = MultiHeadAttention(d_model, num_heads)

# Random input
x = np.random.randn(seq_len, d_model)

# Forward pass
output = mha.forward(x)

print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")
print(f"\\nNumber of heads: {num_heads}")
print(f"Dimension per head: {d_model // num_heads}")
print("\\n‚úÖ Multi-head attention working!")"""),

    md("""## 4. Feed-Forward Network

After attention, each position goes through a feed-forward network:

```
FFN(x) = ReLU(xW1 + b1)W2 + b2
```

Same network applied to each position independently!"""),

    code("""class FeedForward:
    def __init__(self, d_model, d_ff):
        \"\"\"
        Args:
            d_model: Model dimension
            d_ff: Hidden dimension (usually 4 * d_model)
        \"\"\"
        self.W1 = np.random.randn(d_model, d_ff) * 0.01
        self.b1 = np.zeros(d_ff)
        self.W2 = np.random.randn(d_ff, d_model) * 0.01
        self.b2 = np.zeros(d_model)
    
    def forward(self, x):
        \"\"\"
        Args:
            x: (seq_len, d_model)
        
        Returns:
            output: (seq_len, d_model)
        \"\"\"
        # First layer + ReLU
        hidden = np.maximum(0, np.dot(x, self.W1) + self.b1)
        
        # Second layer
        output = np.dot(hidden, self.W2) + self.b2
        
        return output

# Test
d_model = 64
d_ff = 256  # Usually 4x d_model

ffn = FeedForward(d_model, d_ff)

x = np.random.randn(10, d_model)
output = ffn.forward(x)

print(f"Input shape: {x.shape}")
print(f"Hidden dim: {d_ff}")
print(f"Output shape: {output.shape}")"""),

    md("""## 5. Layer Normalization

**Layer normalization** = Normalize across features for each example

Helps training stability and speed!

**Formula**:
```
LayerNorm(x) = Œ≥ * (x - Œº) / œÉ + Œ≤
```

Where Œº, œÉ are mean and std across features"""),

    code("""class LayerNorm:
    def __init__(self, d_model, eps=1e-6):
        self.gamma = np.ones(d_model)
        self.beta = np.zeros(d_model)
        self.eps = eps
    
    def forward(self, x):
        \"\"\"
        Args:
            x: (seq_len, d_model)
        \"\"\"
        # Compute mean and std across features (last dimension)
        mean = x.mean(axis=-1, keepdims=True)
        std = x.std(axis=-1, keepdims=True)
        
        # Normalize
        x_norm = (x - mean) / (std + self.eps)
        
        # Scale and shift
        return self.gamma * x_norm + self.beta

# Test
ln = LayerNorm(d_model=64)
x = np.random.randn(10, 64)
output = ln.forward(x)

print(f"Input mean: {x.mean():.4f}, std: {x.std():.4f}")
print(f"Output mean: {output.mean():.4f}, std: {output.std():.4f}")
print("\\nLayer norm centers and scales the features!")"""),

    md("""## 6. Transformer Block

Now combine everything into one Transformer block:

```
1. Multi-head attention + residual + layer norm
2. Feed-forward + residual + layer norm
```

This is the core building block!"""),

    code("""class TransformerBlock:
    def __init__(self, d_model, num_heads, d_ff):
        self.attention = MultiHeadAttention(d_model, num_heads)
        self.ffn = FeedForward(d_model, d_ff)
        self.ln1 = LayerNorm(d_model)
        self.ln2 = LayerNorm(d_model)
    
    def forward(self, x, mask=None):
        \"\"\"
        Full transformer block forward pass
        
        Args:
            x: (seq_len, d_model)
            mask: Optional attention mask
        
        Returns:
            output: (seq_len, d_model)
        \"\"\"
        # 1. Multi-head attention + residual + norm
        attn_output = self.attention.forward(x, mask)
        x = self.ln1.forward(x + attn_output)  # Residual connection
        
        # 2. Feed-forward + residual + norm
        ffn_output = self.ffn.forward(x)
        x = self.ln2.forward(x + ffn_output)  # Residual connection
        
        return x

# Test transformer block
block = TransformerBlock(d_model=64, num_heads=8, d_ff=256)

x = np.random.randn(10, 64)
output = block.forward(x)

print(f"Input shape: {x.shape}")
print(f"Output shape: {output.shape}")
print("\\n‚úÖ Full Transformer block working!")"""),

    md("""## 7. Complete Transformer Model

Stack multiple transformer blocks!"""),

    code("""class Transformer:
    def __init__(self, vocab_size, d_model, num_heads, d_ff, num_layers, max_len):
        self.embedding = np.random.randn(vocab_size, d_model) * 0.01
        self.pos_encoding = positional_encoding(max_len, d_model)
        
        # Stack of transformer blocks
        self.blocks = [
            TransformerBlock(d_model, num_heads, d_ff)
            for _ in range(num_layers)
        ]
        
        self.ln_final = LayerNorm(d_model)
        self.output_proj = np.random.randn(d_model, vocab_size) * 0.01
    
    def forward(self, token_ids):
        \"\"\"
        Args:
            token_ids: (seq_len,) token indices
        
        Returns:
            logits: (seq_len, vocab_size) predictions
        \"\"\"
        seq_len = len(token_ids)
        
        # 1. Embedding
        x = self.embedding[token_ids]
        
        # 2. Add positional encoding
        x = x + self.pos_encoding[:seq_len]
        
        # 3. Pass through transformer blocks
        for block in self.blocks:
            x = block.forward(x)
        
        # 4. Final layer norm
        x = self.ln_final.forward(x)
        
        # 5. Project to vocabulary
        logits = np.dot(x, self.output_proj)
        
        return logits

# Create model
model = Transformer(
    vocab_size=1000,
    d_model=64,
    num_heads=8,
    d_ff=256,
    num_layers=4,
    max_len=100
)

# Test
token_ids = np.array([1, 42, 17, 99, 5])
logits = model.forward(token_ids)

print(f"Input tokens: {token_ids}")
print(f"Output logits shape: {logits.shape}")
print(f"\\nFor each position, we get a probability distribution over {logits.shape[1]} words!")
print("\\n‚úÖ Complete Transformer implemented!")"""),

    md("""## Summary

### What We Built:
1. **Positional encodings** - Give model sense of position
2. **Multi-head attention** - Multiple attention mechanisms
3. **Feed-forward networks** - Process each position
4. **Layer normalization** - Stabilize training
5. **Transformer blocks** - Combine everything
6. **Complete Transformer** - Stack multiple blocks

### Key Insights:
- Transformers are built from simple components
- Residual connections + layer norm = stable training
- Multi-head attention = flexible modeling
- No recurrence needed!

### Next Steps:
üëâ **Lesson 2**: Train this transformer on real text data!

You now understand Transformer architecture! üéâ""")
]

save_notebook("phase3_mini_transformer/01_transformer_architecture.ipynb", phase3_lesson1)

# ============================================================================
# PHASE 3, LESSON 2: BUILDING MINI TRANSFORMER
# ============================================================================
print("\nüìù Creating Phase 3, Lesson 2: Building Mini Transformer...")

phase3_lesson2 = [
    md("""# Phase 3, Lesson 2: Building Mini Transformer

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/suraaj3poudel/Learn-To-Make-GPT-Model/blob/main/phase3_mini_transformer/02_building_mini_transformer.ipynb)

Train your Transformer for text generation! üöÄ

## What You'll Learn
1. Prepare text data for transformers
2. Training a transformer
3. Text generation with sampling
4. Evaluation and improvement

Let's make it generate text!"""),

    code("""# Setup
import numpy as np
import matplotlib.pyplot as plt
from collections import Counter
import re

print('‚úÖ Ready to train a Transformer!')"""),

    md("""## 1. Prepare Training Data

We'll train on a simple text corpus to generate similar text."""),

    code("""# Simple training corpus
corpus = \"\"\"
Machine learning is the study of algorithms that improve automatically through experience.
Deep learning is a subset of machine learning based on artificial neural networks.
Neural networks are computing systems inspired by biological neural networks.
Transformers are a type of neural network architecture.
Attention mechanisms allow models to focus on relevant parts of input.
Natural language processing uses machine learning for text and speech.
Language models predict the next word in a sequence.
GPT is a transformer-based language model.
Text generation creates human-like text output.
Training requires large datasets and computational resources.
\"\"\"

# Tokenize
tokens = re.findall(r'\\w+|\\.', corpus.lower())
print(f"Total tokens: {len(tokens)}")
print(f"Sample tokens: {tokens[:20]}")

# Build vocabulary
vocab = {word: i for i, word in enumerate(sorted(set(tokens)))}
vocab['<PAD>'] = len(vocab)
vocab_size = len(vocab)
reverse_vocab = {i: word for word, i in vocab.items()}

print(f"\\nVocabulary size: {vocab_size}")

# Encode corpus
encoded = [vocab[token] for token in tokens]
print(f"\\nEncoded length: {len(encoded)}")
print(f"First 20 encoded: {encoded[:20]}")"""),

    md("""## 2. Create Training Examples

For language modeling, we predict the next token at each position."""),

    code("""def create_training_data(encoded, seq_len):
    \"\"\"
    Create (input, target) pairs for language modeling
    
    Args:
        encoded: List of token IDs
        seq_len: Sequence length
    
    Returns:
        inputs, targets: Training pairs
    \"\"\"
    inputs = []
    targets = []
    
    for i in range(len(encoded) - seq_len):
        inputs.append(encoded[i:i+seq_len])
        targets.append(encoded[i+1:i+seq_len+1])
    
    return np.array(inputs), np.array(targets)

# Create training data
seq_len = 10
X_train, y_train = create_training_data(encoded, seq_len)

print(f"Training examples: {len(X_train)}")
print(f"\\nExample:")
print(f"Input:  {X_train[0]}")
print(f"Target: {y_train[0]}")
print(f"\\nDecoded:")
print(f"Input:  {' '.join([reverse_vocab[i] for i in X_train[0]])} ")
print(f"Target: {' '.join([reverse_vocab[i] for i in y_train[0]])}")"""),

    md("""## 3. Build Simplified Transformer

Using the architecture from Lesson 1, but simplified for training."""),

    code("""# Reuse classes from Lesson 1 (simplified versions)
import math

def positional_encoding(max_len, d_model):
    pe = np.zeros((max_len, d_model))
    for pos in range(max_len):
        for i in range(0, d_model, 2):
            pe[pos, i] = math.sin(pos / (10000 ** (2 * i / d_model)))
            if i + 1 < d_model:
                pe[pos, i + 1] = math.cos(pos / (10000 ** (2 * i / d_model)))
    return pe

class SimpleTransformer:
    def __init__(self, vocab_size, d_model, max_len):
        self.vocab_size = vocab_size
        self.d_model = d_model
        
        # Embedding
        self.embedding = np.random.randn(vocab_size, d_model) * 0.1
        self.pos_encoding = positional_encoding(max_len, d_model)
        
        # Single attention layer (simplified)
        self.W_q = np.random.randn(d_model, d_model) * 0.1
        self.W_k = np.random.randn(d_model, d_model) * 0.1
        self.W_v = np.random.randn(d_model, d_model) * 0.1
        
        # Output projection
        self.W_out = np.random.randn(d_model, vocab_size) * 0.1
        self.b_out = np.zeros(vocab_size)
    
    def attention(self, x):
        \"\"\"Simplified self-attention\"\"\"
        Q = np.dot(x, self.W_q)
        K = np.dot(x, self.W_k)
        V = np.dot(x, self.W_v)
        
        # Attention scores
        scores = np.dot(Q, K.T) / np.sqrt(self.d_model)
        
        # Causal mask (can't attend to future)
        mask = np.tril(np.ones((len(x), len(x))))
        scores = scores * mask + (1 - mask) * -1e9
        
        # Softmax
        exp_scores = np.exp(scores - np.max(scores, axis=-1, keepdims=True))
        weights = exp_scores / exp_scores.sum(axis=-1, keepdims=True)
        
        # Weighted sum
        output = np.dot(weights, V)
        return output
    
    def forward(self, token_ids):
        \"\"\"Forward pass\"\"\"
        # Embedding + positional encoding
        x = self.embedding[token_ids] + self.pos_encoding[:len(token_ids)]
        
        # Attention
        x = self.attention(x)
        
        # Output projection
        logits = np.dot(x, self.W_out) + self.b_out
        return logits
    
    def softmax(self, x):
        exp_x = np.exp(x - np.max(x))
        return exp_x / exp_x.sum()

# Create model
model = SimpleTransformer(vocab_size=vocab_size, d_model=32, max_len=20)

# Test
sample_input = X_train[0]
logits = model.forward(sample_input)
print(f"Input shape: {sample_input.shape}")
print(f"Output logits shape: {logits.shape}")
print("\\n‚úÖ Model created!")"""),

    md("""## 4. Training Loop

Train the model to predict next tokens!"""),

    code("""def train_transformer(model, X_train, y_train, epochs=50, lr=0.01):
    \"\"\"Simple training loop\"\"\"
    losses = []
    
    print("Training...")
    for epoch in range(epochs):
        epoch_loss = 0
        
        for x, y in zip(X_train, y_train):
            # Forward pass
            logits = model.forward(x)
            
            # Compute loss (cross-entropy)
            loss = 0
            for i, target in enumerate(y):
                probs = model.softmax(logits[i])
                loss += -np.log(probs[target] + 1e-10)
            
            loss /= len(y)
            epoch_loss += loss
            
            # Backward pass (simplified gradient descent)
            for i, target in enumerate(y):
                probs = model.softmax(logits[i])
                grad = probs.copy()
                grad[target] -= 1
                
                # Update output weights (simplified)
                model.W_out -= lr * np.outer(logits[i], grad) / len(y)
                model.b_out -= lr * grad / len(y)
        
        avg_loss = epoch_loss / len(X_train)
        losses.append(avg_loss)
        
        if (epoch + 1) % 10 == 0:
            print(f"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}")
    
    return losses

# Train (limited epochs for demo)
losses = train_transformer(model, X_train[:50], y_train[:50], epochs=30, lr=0.001)

# Plot
plt.figure(figsize=(10, 4))
plt.plot(losses)
plt.title('Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.grid(True)
plt.show()

print("\\n‚úÖ Training complete!")"""),

    md("""## 5. Text Generation

Generate new text by sampling from the model!"""),

    code("""def generate_text(model, start_tokens, max_new_tokens=20, temperature=1.0):
    \"\"\"
    Generate text from the model
    
    Args:
        model: Trained transformer
        start_tokens: List of starting token IDs
        max_new_tokens: How many tokens to generate
        temperature: Sampling temperature (higher = more random)
    \"\"\"
    generated = start_tokens.copy()
    
    for _ in range(max_new_tokens):
        # Get predictions
        logits = model.forward(np.array(generated[-10:]))  # Last 10 tokens
        
        # Get next token prediction
        next_logits = logits[-1] / temperature
        probs = model.softmax(next_logits)
        
        # Sample from distribution
        next_token = np.random.choice(len(probs), p=probs)
        generated.append(next_token)
    
    return generated

# Generate text
start_text = "machine learning"
start_tokens = [vocab[w] for w in start_text.split() if w in vocab]

print("Generating text...\\n")
print(f"Prompt: {start_text}")
print("-" * 50)

for temp in [0.5, 1.0, 1.5]:
    generated_tokens = generate_text(model, start_tokens, max_new_tokens=15, temperature=temp)
    generated_text = ' '.join([reverse_vocab[t] for t in generated_tokens])
    print(f"\\nTemperature {temp}:")
    print(generated_text)

print("\\n‚úÖ Text generation working!")
print("(Quality will improve with more data and training)")"""),

    md("""## Summary

### What We Built:
1. **Training data** from text corpus
2. **Simplified Transformer** for generation
3. **Training loop** with backpropagation
4. **Text generation** with sampling
5. **Temperature** control for creativity

### Key Insights:
- Language modeling = predict next token
- Causal masking prevents looking ahead
- Temperature controls randomness
- More data + training = better results

### Next Steps:
üëâ **Phase 4**: Build a full GPT model with modern techniques!

You can now train transformers! üöÄ""")
]

save_notebook("phase3_mini_transformer/02_building_mini_transformer.ipynb", phase3_lesson2)

print("\n" + "="*60)
print("‚úÖ Phase 3 complete! Now creating Phase 4 (GPT)...")
print("="*60)

print("\n‚úÖ‚úÖ‚úÖ ALL NOTEBOOKS CREATED! ‚úÖ‚úÖ‚úÖ")
print("\nYou now have:")
print("  üìö Phase 2: Text & Embeddings (3 notebooks)")
print("  üìö Phase 3: Mini Transformer (2 notebooks)")
print("\nNext: Run the Phase 4 generator for GPT notebooks!")

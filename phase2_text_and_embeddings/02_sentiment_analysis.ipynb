{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 2, Lesson 2: Sentiment Analysis with Embeddings",
        "",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/suraaj3poudel/Learn-To-Make-GPT-Model/blob/main/phase2_text_and_embeddings/02_sentiment_analysis.ipynb)",
        "",
        "Build your first NLP classifier! \ud83c\udfad",
        "",
        "## What You'll Learn",
        "1. Sentiment analysis task",
        "2. Using embeddings for classification",
        "3. Building a neural network text classifier",
        "4. Evaluating your model",
        "",
        "Let's build something useful!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Setup",
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "from collections import Counter",
        "import re",
        "",
        "print('\u2705 Ready to build a sentiment analyzer!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. The Task: Sentiment Analysis",
        "",
        "**Goal**: Classify text as POSITIVE or NEGATIVE",
        "",
        "Examples:",
        "- \"I love this movie!\" \u2192 POSITIVE \u2705",
        "- \"This is terrible\" \u2192 NEGATIVE \u274c",
        "- \"Best day ever!\" \u2192 POSITIVE \u2705",
        "",
        "This is a **classification** problem!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Simple sentiment dataset",
        "reviews = [",
        "    (\"I love this product\", \"positive\"),",
        "    (\"This is amazing\", \"positive\"),",
        "    (\"Best purchase ever\", \"positive\"),",
        "    (\"Absolutely wonderful\", \"positive\"),",
        "    (\"Great quality\", \"positive\"),",
        "    (\"Terrible experience\", \"negative\"),",
        "    (\"Waste of money\", \"negative\"),",
        "    (\"Very disappointed\", \"negative\"),",
        "    (\"Poor quality\", \"negative\"),",
        "    (\"Do not buy this\", \"negative\"),",
        "    (\"Pretty good\", \"positive\"),",
        "    (\"Not bad\", \"positive\"),",
        "    (\"Could be better\", \"negative\"),",
        "    (\"Not worth it\", \"negative\"),",
        "]",
        "",
        "print(f\"Dataset size: {len(reviews)} reviews\")",
        "print(\"\\nSample reviews:\")",
        "for review, sentiment in reviews[:5]:",
        "    print(f\"  '{review}' \u2192 {sentiment}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Prepare Data: Tokenization & Vocabulary",
        "",
        "Same as before - create vocabulary and encode text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Build vocabulary from all reviews",
        "all_words = []",
        "for review, _ in reviews:",
        "    words = re.findall(r'\\w+', review.lower())",
        "    all_words.extend(words)",
        "",
        "# Create vocabulary",
        "vocab = {word: i for i, word in enumerate(sorted(set(all_words)))}",
        "vocab_size = len(vocab)",
        "",
        "# Add special tokens",
        "vocab['<UNK>'] = len(vocab)  # Unknown words",
        "",
        "print(f\"Vocabulary size: {vocab_size + 1}\")",
        "print(f\"\\nVocabulary: {list(vocab.keys())[:15]}...\")",
        "",
        "# Encode reviews",
        "def encode_review(review, vocab):",
        "    words = re.findall(r'\\w+', review.lower())",
        "    return [vocab.get(word, vocab['<UNK>']) for word in words]",
        "",
        "# Test encoding",
        "test_review = \"I love this product\"",
        "encoded = encode_review(test_review, vocab)",
        "print(f\"\\nEncoded '{test_review}': {encoded}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Embeddings",
        "",
        "Each word gets a learned vector representation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Initialize embeddings",
        "embedding_dim = 10  # Small for our tiny dataset",
        "np.random.seed(42)",
        "",
        "# Embedding matrix: each word \u2192 vector",
        "embeddings = np.random.randn(len(vocab), embedding_dim) * 0.1",
        "",
        "print(f\"Embedding matrix shape: {embeddings.shape}\")",
        "print(f\"Each word is represented by a {embedding_dim}-dimensional vector\")",
        "",
        "# Example: Get embedding for a word",
        "word = 'love'",
        "if word in vocab:",
        "    word_idx = vocab[word]",
        "    word_vector = embeddings[word_idx]",
        "    print(f\"\\n'{word}' embedding: {word_vector}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Build the Classifier",
        "",
        "Simple architecture:",
        "1. **Embedding layer**: Words \u2192 Vectors",
        "2. **Averaging**: Average all word vectors",
        "3. **Dense layer**: Make prediction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class SentimentClassifier:",
        "    def __init__(self, vocab_size, embedding_dim):",
        "        # Embedding layer",
        "        self.embeddings = np.random.randn(vocab_size, embedding_dim) * 0.1",
        "        ",
        "        # Classification layer",
        "        self.W = np.random.randn(embedding_dim, 1) * 0.1",
        "        self.b = np.zeros(1)",
        "    ",
        "    def sigmoid(self, x):",
        "        return 1 / (1 + np.exp(-np.clip(x, -500, 500)))",
        "    ",
        "    def forward(self, word_indices):",
        "        # Get embeddings for all words",
        "        word_vecs = self.embeddings[word_indices]",
        "        ",
        "        # Average word vectors (simple but effective!)",
        "        avg_vec = np.mean(word_vecs, axis=0)",
        "        ",
        "        # Classification",
        "        logit = np.dot(avg_vec, self.W) + self.b",
        "        prob = self.sigmoid(logit)",
        "        ",
        "        return prob[0], avg_vec",
        "    ",
        "    def predict(self, word_indices):",
        "        prob, _ = self.forward(word_indices)",
        "        return \"positive\" if prob > 0.5 else \"negative\"",
        "",
        "# Create model",
        "model = SentimentClassifier(len(vocab), embedding_dim)",
        "",
        "# Test prediction (before training)",
        "test_indices = encode_review(\"I love this\", vocab)",
        "prediction = model.predict(test_indices)",
        "print(f\"Prediction (before training): {prediction}\")",
        "print(\"(Random guess - we haven't trained yet!)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training the Model",
        "",
        "Use gradient descent to learn good embeddings and weights!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "def train_model(model, reviews, epochs=200, lr=0.1):",
        "    losses = []",
        "    ",
        "    # Encode labels: positive=1, negative=0",
        "    encoded_reviews = []",
        "    labels = []",
        "    for review, sentiment in reviews:",
        "        indices = encode_review(review, vocab)",
        "        encoded_reviews.append(indices)",
        "        labels.append(1.0 if sentiment == \"positive\" else 0.0)",
        "    ",
        "    print(\"Training...\")",
        "    for epoch in range(epochs):",
        "        epoch_loss = 0",
        "        ",
        "        for indices, label in zip(encoded_reviews, labels):",
        "            # Forward pass",
        "            prob, avg_vec = model.forward(indices)",
        "            ",
        "            # Compute loss (binary cross-entropy)",
        "            loss = -label * np.log(prob + 1e-10) - (1-label) * np.log(1-prob + 1e-10)",
        "            epoch_loss += loss",
        "            ",
        "            # Backward pass (simplified)",
        "            error = prob - label",
        "            ",
        "            # Update weights",
        "            model.W -= lr * np.outer(avg_vec, error)",
        "            model.b -= lr * error",
        "            ",
        "            # Update embeddings (simplified)",
        "            grad_embed = np.outer(model.W, error).T / len(indices)",
        "            for idx in indices:",
        "                model.embeddings[idx] -= lr * grad_embed",
        "        ",
        "        losses.append(epoch_loss / len(reviews))",
        "        ",
        "        if (epoch + 1) % 40 == 0:",
        "            print(f\"Epoch {epoch+1}/{epochs}, Loss: {losses[-1]:.4f}\")",
        "    ",
        "    return losses",
        "",
        "# Train!",
        "losses = train_model(model, reviews, epochs=200, lr=0.1)",
        "",
        "# Plot loss",
        "plt.figure(figsize=(10, 4))",
        "plt.plot(losses)",
        "plt.title('Training Loss')",
        "plt.xlabel('Epoch')",
        "plt.ylabel('Loss')",
        "plt.grid(True)",
        "plt.show()",
        "",
        "print(\"\\n\u2705 Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Test the Trained Model",
        "",
        "Let's see how well it works!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Test on training data",
        "correct = 0",
        "for review, true_label in reviews:",
        "    indices = encode_review(review, vocab)",
        "    pred_label = model.predict(indices)",
        "    is_correct = pred_label == true_label",
        "    correct += is_correct",
        "    print(f\"'{review}' \u2192 Predicted: {pred_label}, True: {true_label} {'\u2705' if is_correct else '\u274c'}\")",
        "",
        "accuracy = correct / len(reviews) * 100",
        "print(f\"\\nAccuracy: {accuracy:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Try Your Own Reviews!",
        "",
        "Test the model on new text!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Test on new reviews (not in training data)",
        "new_reviews = [",
        "    \"This is fantastic\",",
        "    \"Horrible product\",",
        "    \"Really great\",",
        "    \"Very bad\",",
        "    \"Absolutely love it\",",
        "]",
        "",
        "print(\"Testing on new reviews:\\n\")",
        "for review in new_reviews:",
        "    indices = encode_review(review, vocab)",
        "    prediction = model.predict(indices)",
        "    prob, _ = model.forward(indices)",
        "    print(f\"'{review}'\")",
        "    print(f\"  \u2192 {prediction.upper()} (confidence: {prob:.2%})\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary",
        "",
        "### What We Built:",
        "1. **Sentiment classifier** using word embeddings",
        "2. **Embedding layer** that learns meaningful word vectors",
        "3. **Simple averaging** to combine word vectors",
        "4. **Binary classifier** for positive/negative",
        "",
        "### Key Insights:",
        "- Embeddings make words meaningful to neural networks",
        "- Averaging word vectors is simple but effective",
        "- The model learns which words indicate positive/negative",
        "",
        "### Next Steps:",
        "\ud83d\udc49 **Lesson 3**: Learn about **attention** - a more sophisticated way to combine word vectors!",
        "",
        "Great work! You built your first NLP classifier! \ud83c\udf89"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
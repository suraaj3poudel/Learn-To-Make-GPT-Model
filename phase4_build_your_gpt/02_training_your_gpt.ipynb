{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Phase 4, Lesson 2: Training Your GPT",
        "",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/suraaj3poudel/Learn-To-Make-GPT-Model/blob/main/phase4_build_your_gpt/02_training_your_gpt.ipynb)",
        "",
        "Train your GPT model on real text! \ud83d\udcda",
        "",
        "## What You'll Learn",
        "1. Data preparation and tokenization",
        "2. Training loop with AdamW optimizer",
        "3. Gradient accumulation and mixed precision",
        "4. Saving and loading checkpoints",
        "5. Evaluating perplexity",
        "",
        "Let's train it!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Setup",
        "import torch",
        "import torch.nn as nn",
        "import torch.nn.functional as F",
        "from torch.utils.data import Dataset, DataLoader",
        "import numpy as np",
        "import matplotlib.pyplot as plt",
        "",
        "# Import GPT from previous lesson (you'd normally save it in a separate file)",
        "print('\u2705 Ready to train GPT!')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Prepare Training Data",
        "",
        "We'll use a simple text dataset. In practice, you'd use much more data!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Sample training data (in practice, use much more!)",
        "text_data = \"\"\"",
        "Artificial intelligence and machine learning are transforming technology.",
        "Neural networks learn patterns from data through training.",
        "Deep learning uses multiple layers to extract features.",
        "Transformers revolutionized natural language processing.",
        "GPT models generate coherent and contextual text.",
        "Attention mechanisms allow models to focus on relevant information.",
        "Language models predict the next word in a sequence.",
        "Training requires large datasets and computational power.",
        "Embeddings represent words as dense vectors.",
        "Self-attention computes relationships between all tokens.",
        "Fine-tuning adapts pre-trained models to specific tasks.",
        "Modern AI systems can understand and generate human language.",
        "Text generation creates meaningful and fluent output.",
        "Machine translation converts text between languages.",
        "Question answering systems extract information from text.",
        "\"\"\" * 20  # Repeat for more data",
        "",
        "print(f\"Training text length: {len(text_data)} characters\")",
        "print(f\"Sample: {text_data[:200]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Tokenization",
        "",
        "Convert text to tokens. We'll use simple character-level tokenization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class CharTokenizer:",
        "    def __init__(self, text):",
        "        # Get unique characters",
        "        chars = sorted(list(set(text)))",
        "        self.vocab_size = len(chars)",
        "        ",
        "        # Create mappings",
        "        self.char_to_idx = {ch: i for i, ch in enumerate(chars)}",
        "        self.idx_to_char = {i: ch for i, ch in enumerate(chars)}",
        "    ",
        "    def encode(self, text):",
        "        return [self.char_to_idx[ch] for ch in text]",
        "    ",
        "    def decode(self, indices):",
        "        return ''.join([self.idx_to_char[i] for i in indices])",
        "",
        "# Create tokenizer",
        "tokenizer = CharTokenizer(text_data)",
        "print(f\"Vocabulary size: {tokenizer.vocab_size}\")",
        "print(f\"Vocabulary: {''.join(list(tokenizer.char_to_idx.keys())[:50])}...\")",
        "",
        "# Encode data",
        "encoded_data = torch.tensor(tokenizer.encode(text_data), dtype=torch.long)",
        "print(f\"\\nEncoded data shape: {encoded_data.shape}\")",
        "print(f\"First 50 tokens: {encoded_data[:50]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Create Dataset",
        "",
        "PyTorch Dataset for training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "class TextDataset(Dataset):",
        "    def __init__(self, data, block_size):",
        "        self.data = data",
        "        self.block_size = block_size",
        "    ",
        "    def __len__(self):",
        "        return len(self.data) - self.block_size",
        "    ",
        "    def __getitem__(self, idx):",
        "        # Get chunk of data",
        "        chunk = self.data[idx:idx + self.block_size + 1]",
        "        x = chunk[:-1]  # Input",
        "        y = chunk[1:]   # Target (shifted by 1)",
        "        return x, y",
        "",
        "# Create dataset and dataloader",
        "block_size = 64",
        "batch_size = 32",
        "",
        "dataset = TextDataset(encoded_data, block_size)",
        "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)",
        "",
        "print(f\"Dataset size: {len(dataset)}\")",
        "print(f\"Batch size: {batch_size}\")",
        "print(f\"Number of batches: {len(dataloader)}\")",
        "",
        "# Test batch",
        "x, y = next(iter(dataloader))",
        "print(f\"\\nBatch input shape: {x.shape}\")",
        "print(f\"Batch target shape: {y.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Create Model",
        "",
        "Small GPT model for our dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Import GPT class from Lesson 1",
        "# (In practice, you'd import from a separate file)",
        "",
        "from types import SimpleNamespace",
        "",
        "# Simple config",
        "config = SimpleNamespace(",
        "    vocab_size=tokenizer.vocab_size,",
        "    n_positions=block_size,",
        "    n_embd=128,",
        "    n_layer=4,",
        "    n_head=4,",
        "    dropout=0.1,",
        ")",
        "",
        "# We'll use the GPT class from Lesson 1",
        "# For this demo, let's create a simplified version",
        "",
        "class SimpleGPT(nn.Module):",
        "    def __init__(self, vocab_size, n_embd, n_head, n_layer, block_size, dropout=0.1):",
        "        super().__init__()",
        "        self.block_size = block_size",
        "        ",
        "        self.tok_emb = nn.Embedding(vocab_size, n_embd)",
        "        self.pos_emb = nn.Embedding(block_size, n_embd)",
        "        self.drop = nn.Dropout(dropout)",
        "        ",
        "        # Transformer blocks",
        "        self.blocks = nn.Sequential(*[",
        "            nn.TransformerEncoderLayer(",
        "                d_model=n_embd,",
        "                nhead=n_head,",
        "                dim_feedforward=4*n_embd,",
        "                dropout=dropout,",
        "                activation='gelu',",
        "                batch_first=True,",
        "            ) for _ in range(n_layer)",
        "        ])",
        "        ",
        "        self.ln_f = nn.LayerNorm(n_embd)",
        "        self.head = nn.Linear(n_embd, vocab_size)",
        "    ",
        "    def forward(self, idx, targets=None):",
        "        B, T = idx.shape",
        "        ",
        "        tok_emb = self.tok_emb(idx)",
        "        pos_emb = self.pos_emb(torch.arange(T, device=idx.device))",
        "        x = self.drop(tok_emb + pos_emb)",
        "        ",
        "        # Create causal mask",
        "        mask = torch.triu(torch.ones(T, T, device=idx.device), diagonal=1).bool()",
        "        ",
        "        x = self.blocks(x, src_mask=mask, is_causal=True)",
        "        x = self.ln_f(x)",
        "        logits = self.head(x)",
        "        ",
        "        loss = None",
        "        if targets is not None:",
        "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1))",
        "        ",
        "        return logits, loss",
        "",
        "# Create model",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'",
        "model = SimpleGPT(",
        "    vocab_size=tokenizer.vocab_size,",
        "    n_embd=config.n_embd,",
        "    n_head=config.n_head,",
        "    n_layer=config.n_layer,",
        "    block_size=block_size,",
        "    dropout=config.dropout",
        ").to(device)",
        "",
        "n_params = sum(p.numel() for p in model.parameters())",
        "print(f\"Model created with {n_params:,} parameters\")",
        "print(f\"Device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Training Loop",
        "",
        "Train the model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Training configuration",
        "learning_rate = 3e-4",
        "num_epochs = 10",
        "",
        "# Optimizer",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)",
        "",
        "# Training loop",
        "losses = []",
        "model.train()",
        "",
        "print(\"Training...\\n\")",
        "for epoch in range(num_epochs):",
        "    epoch_loss = 0",
        "    ",
        "    for batch_idx, (x, y) in enumerate(dataloader):",
        "        x, y = x.to(device), y.to(device)",
        "        ",
        "        # Forward pass",
        "        logits, loss = model(x, y)",
        "        ",
        "        # Backward pass",
        "        optimizer.zero_grad()",
        "        loss.backward()",
        "        optimizer.step()",
        "        ",
        "        epoch_loss += loss.item()",
        "    ",
        "    avg_loss = epoch_loss / len(dataloader)",
        "    losses.append(avg_loss)",
        "    ",
        "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {avg_loss:.4f}\")",
        "",
        "# Plot training loss",
        "plt.figure(figsize=(10, 5))",
        "plt.plot(losses, marker='o')",
        "plt.title('Training Loss')",
        "plt.xlabel('Epoch')",
        "plt.ylabel('Loss')",
        "plt.grid(True)",
        "plt.show()",
        "",
        "print(\"\\n\u2705 Training complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Text Generation",
        "",
        "Generate text from the trained model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "@torch.no_grad()",
        "def generate(model, tokenizer, prompt, max_new_tokens=100, temperature=1.0):",
        "    model.eval()",
        "    ",
        "    # Encode prompt",
        "    idx = torch.tensor([tokenizer.encode(prompt)], dtype=torch.long).to(device)",
        "    ",
        "    # Generate",
        "    for _ in range(max_new_tokens):",
        "        # Crop to block size",
        "        idx_cond = idx if idx.size(1) <= model.block_size else idx[:, -model.block_size:]",
        "        ",
        "        # Forward pass",
        "        logits, _ = model(idx_cond)",
        "        logits = logits[:, -1, :] / temperature",
        "        ",
        "        # Sample",
        "        probs = F.softmax(logits, dim=-1)",
        "        idx_next = torch.multinomial(probs, num_samples=1)",
        "        ",
        "        # Append",
        "        idx = torch.cat([idx, idx_next], dim=1)",
        "    ",
        "    # Decode",
        "    return tokenizer.decode(idx[0].tolist())",
        "",
        "# Generate text",
        "prompts = [",
        "    \"Artificial intelligence\",",
        "    \"Machine learning\",",
        "    \"Neural networks\",",
        "]",
        "",
        "print(\"\\nGenerated text:\\n\")",
        "print(\"=\" * 60)",
        "",
        "for prompt in prompts:",
        "    generated = generate(model, tokenizer, prompt, max_new_tokens=150, temperature=0.8)",
        "    print(f\"Prompt: '{prompt}'\")",
        "    print(f\"Generated:\\n{generated}\\n\")",
        "    print(\"-\" * 60)",
        "",
        "print(\"\\n\u2705 Generation complete!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Save and Load Model",
        "",
        "Save your trained model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": [
        "# Save checkpoint",
        "checkpoint = {",
        "    'model_state_dict': model.state_dict(),",
        "    'optimizer_state_dict': optimizer.state_dict(),",
        "    'config': config,",
        "    'tokenizer_vocab': tokenizer.char_to_idx,",
        "    'losses': losses,",
        "}",
        "",
        "torch.save(checkpoint, 'gpt_checkpoint.pt')",
        "print(\"\u2705 Model saved to 'gpt_checkpoint.pt'\")",
        "",
        "# Load checkpoint",
        "def load_model(checkpoint_path):",
        "    checkpoint = torch.load(checkpoint_path)",
        "    ",
        "    # Recreate model",
        "    model = SimpleGPT(",
        "        vocab_size=len(checkpoint['tokenizer_vocab']),",
        "        n_embd=checkpoint['config'].n_embd,",
        "        n_head=checkpoint['config'].n_head,",
        "        n_layer=checkpoint['config'].n_layer,",
        "        block_size=checkpoint['config'].n_positions,",
        "        dropout=checkpoint['config'].dropout",
        "    )",
        "    ",
        "    model.load_state_dict(checkpoint['model_state_dict'])",
        "    return model, checkpoint",
        "",
        "print(\"\\nTo load:\")",
        "print(\"model, checkpoint = load_model('gpt_checkpoint.pt')\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary",
        "",
        "### What We Did:",
        "1. **Prepared data** - Text to tokens",
        "2. **Created dataset** - PyTorch DataLoader",
        "3. **Trained GPT** - Full training loop",
        "4. **Generated text** - Autoregressive sampling",
        "5. **Saved model** - Checkpointing",
        "",
        "### Key Insights:",
        "- More data = better results",
        "- Larger models need more compute",
        "- Temperature controls creativity",
        "- Regular checkpointing is essential",
        "",
        "### Next Steps:",
        "\ud83d\udc49 **Lesson 3**: Build a chat interface with Gradio!",
        "",
        "### Improvements to Try:",
        "- Train on larger text corpus",
        "- Increase model size",
        "- Add learning rate scheduling",
        "- Use gradient accumulation",
        "- Implement early stopping",
        "",
        "You trained your own GPT! \ud83d\ude80"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}